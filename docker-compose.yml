version: '3.8'

services:
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: api_service
    environment:
      - AI_OLLAMA_MODEL=nemotron-mini #mistral  # Set environment variable for the container
      - LLM_CLIENT_BASE=http://ollama:11434
    ports:
      - "8000:8000"  # Expose API on port 8000
    networks:
      - app-network
    volumes:
      - ./user_data:/user_data

  ui:
    build:
      context: ./streamlit/
      dockerfile: Dockerfile
    container_name: ui_service
    ports:
      - "8501:8501"  # Expose UI on port 8501
    environment:
      API_BASE: http://api:8000
    depends_on:
      - api
      - gradio_api
    networks:
      - app-network

  ollama:
    build:
      context: .  # Use the current directory as the build context
      dockerfile: llms/ollama/dockerfile  # Specify the Dockerfile to use for building the image
    ports:
      - "11434:11434"  # Map port 11434 on the host to port 11434 in the container
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia  # Specify the driver to use for GPU access
              count: 1  # Limit access to one GPU
              capabilities: [gpu]  # Specify that the container needs GPU access
    networks:
      - app-network

  gradio_api:
    build:
      context: ./gradio/
      dockerfile: Dockerfile
    container_name: gradio_api_service
    ports:
      - "8001:8001"  # Expose Gradio API on port 8001
      - "7860-7890:7860-7890"  # Open a range of ports for Gradio
    depends_on:
      - api
    networks:
      - app-network
    environment:
      - API_BASE=http://api:8000
      - GRADIO_SERVER_NAME=0.0.0.0

networks:
  app-network:
    driver: bridge
